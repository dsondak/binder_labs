{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: `keras` and Neural Networks\n",
    "\n",
    "## Date:  Saturday, October 20th 2018\n",
    "\n",
    "#### Authors: David Sondak and Pavlos Protopapas\n",
    "\n",
    "In lab 1, we created our own neural network by writing some simple `python` functions.  We focused on a regression problem wherein we tried to learn a function.  We practiced using the logistic activation function in a network with multiple nodes, but a single hidden layer.  Some of the key observations were:\n",
    "* Increasing the number of nodes allows us to represent more complicated functions  \n",
    "* The weights and biases have a very big impact on the solution\n",
    "* Finding the \"correct\" weights and biases is really hard to do manually\n",
    "* There must be a better method for determining the weights and biases automatically\n",
    "\n",
    "We also didn't assess the effects of different activation functions or different network depths.\n",
    "\n",
    "Today, we will repeat some of the analysis from last time, but in the lab we will use [`keras`](https://keras.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Architectures Encoding Functions\n",
    "\n",
    "**Motivating Question:** What exact kind of mathematical/statistical object is encoded by a neural network?\n",
    "\n",
    "For a network with the following architecture (a single output and one hidden layer), write the closed form expression for the function $f$ represented by the network.\n",
    "\n",
    "Assume that the activation function at the output node is the identity funciton. Use the following notation:\n",
    "- let $\\mathbf{x}\\in \\mathbb{R}^D$ be the input; let the components of $\\mathbf{x}$ be indexed by $d$\n",
    "- let $H$ be the total number of hidden nodes, indexed by $h$\n",
    "- let $\\phi_h$ be the activation function at the hidden node $h$\n",
    "- let $\\mathbf{u}_h \\in \\mathbb{R}^D$ be the weights connecting the input to the $h$-th hidden node\n",
    "- let $\\mathbf{a} \\in \\mathbb{R}^H$ be the bias for the hidden layer\n",
    "- let $\\mathbf{v} \\in \\mathbb{R}^H$ be the weights connecting the hidden nodes to the output\n",
    "- let $b \\in \\mathbb{R}$ be the bias for the output layer\n",
    "\n",
    "![](figs/single_hidden_layer.jpg)\n",
    "\n",
    "For each hidden node $h$, a linear combination of the input, $\\mathbf{u}_h^\\top \\mathbf{x} + \\mathbf{a}$, is transformed by the activation function $\\phi_h$. Thus, the output of each hidden node $h$ is\n",
    "$$\n",
    "\\phi_h(\\mathbf{u}_h^\\top \\mathbf{x} + \\mathbf{a}).\n",
    "$$\n",
    "At the output node, a linear combination of the hidden nodes is taken. Since the activation function here is the identity, the final output of the MLP is\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(\\mathbf{x}) = b + \\sum_{h=1}^H v_h\\phi_h(\\mathbf{u}_h^\\top \\mathbf{x} + \\mathbf{a}).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Regression\n",
    "\n",
    "**Motivating Question:** We saw in Part I that each neural network represents a function that depends on our choice of activation function for each node. In practice, we choose the same activation function for all nodes, from a small set of simple functions. It makes sense to ask just how expressive such networks can be. That is, ***can any function be reasonably approximated by a neural network?*** For a fixed function, ***what kind of archicture do we need in order to approximate it?*** Deep (multiple layers) or wide (many hidden nodes in one layer)?\n",
    "\n",
    "![](figs/activation-functions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get to work.  First, we import our basic libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Keras` Basics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Installation\n",
    "\n",
    "If you haven't already, please install `Keras` using the instructions found at [https://keras.io/#installation](https://keras.io/#installation)\n",
    "\n",
    "Choose the TensorFlow installation instructions (found at [https://www.tensorflow.org/install/](https://www.tensorflow.org/install/) ).\n",
    "\n",
    "Note the following:\n",
    "\n",
    "* cuDNN is only required if your machine has an NVidia graphics card (GPU)\n",
    "* For this tutorial, HDF5 and h5py are not required\n",
    "* graphviz and pydot are not required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Neural Networks\n",
    "Let's try to redo the problem from last week.  Recall that we had a function $$f\\left(x\\right) = \\exp\\left(-x^{2}\\right)$$ and we wanted to use a neural network to approximate that function.  This week, we will use `keras` to do the true optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the necessary `keras` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up a network.  Let's do a single hidden layer with two neurons.  Last week we tried to manually tune the weights and things didn't work so well.\n",
    "\n",
    "Before we get started, we need to create some data.  We will generate data points from an underlying function (here the Guassian).  Then we will use the `sklearn` `train_test_split` method to split the dataset into training and testing portions.  Remember that we train a machine learning algorithm on the training set and then assess the algorithm's performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_samples = 1000 # set the number of samples to take for each toy dataset\n",
    "test_size = 0.3 # set the proportion of toy data to hold out for testing\n",
    "random_seed = 1 # set the random seed to make the experiment reproducible \n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# define a function\n",
    "f = lambda x: np.exp(-x * x)\n",
    "X = np.random.permutation(np.linspace(-5, 5, n_samples)) # choose some points from the function - this is our toy dataset \n",
    "Y = f(X)\n",
    "\n",
    "# create training and testing data from this set of points\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a neural network model with `keras`.  We're going to use a single layer and just $2$ neurons in that layer.  To be consisten with last week, we will start with the sigmoid activation function.  We also choose a linear output layer like last time.  The loss function is selected to be the mean squared error.  In addition to these choices we must also specify our initial weights as well as the optimization method that will be used to minimize the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 2 # number of nodes in the layer\n",
    "input_dim = 1 # input dimension: just x\n",
    "\n",
    "model = models.Sequential() # create sequential multi-layer perceptron\n",
    "\n",
    "# layer 0, our hidden layer\n",
    "model.add(layers.Dense(H, input_dim=input_dim, \n",
    "                kernel_initializer='normal', \n",
    "                activation='sigmoid'))\n",
    "# layer 1\n",
    "model.add(layers.Dense(1, kernel_initializer='normal', \n",
    "                activation='linear')) \n",
    "\n",
    "# configure the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is now ready to use.  We haven't trained it yet, but we'll do that now using the `fit` method.  Notice that we also need to specify the *batch size* for the stochastic gradient decent algorithm as well as the number of epochs to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model_history = model.fit(X_train, Y_train, batch_size=128, epochs=500, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!  We've trained a model.  Now it's time to explore the results.  The first thing to notice is the value of the loss function.  Does it look okay to you?\n",
    "\n",
    "Let's see what the results look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our model to predict in the range we want\n",
    "X_range = np.linspace(-5, 5, 1000)\n",
    "y_pred = model.predict(X_range)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14,8))\n",
    "ax.scatter(X_train, Y_train, label='Training data')\n",
    "ax.plot(X_range, y_pred, lw=4, color='r', label='MLP with one hidden layer')\n",
    "ax.set_xlabel(r'$X$', fontsize=20)\n",
    "ax.set_ylabel(r'$Y$', fontsize=20)\n",
    "ax.set_title('Toy data set for regression', fontsize=24)\n",
    "ax.tick_params(labelsize=20)\n",
    "\n",
    "ax.legend(loc=1, fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do things look?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Explore the following:\n",
    "* Different activation functions\n",
    "  - ReLU\n",
    "  - $tanh$\n",
    "  - [Something else?](https://keras.io/activations/)\n",
    "* Change the number of neurons\n",
    "  - Increase it from $2$ to $4$ at first\n",
    "  - What number seems to work well?\n",
    "* Try to play with the number of epochs and the batch size.  Do these have an impact?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Plot the loss function as a function of the epochs.\n",
    "\n",
    "#### Hint\n",
    "You can access the loss function values with the command:\n",
    "```python\n",
    "model_history.history['loss']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performance\n",
    "How good is the model?  We can compute the $R^{2}$ score to get a sense of the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the training and testing performance of your model \n",
    "# note: you should extract and check both the loss function and your evaluation metric\n",
    "from sklearn.metrics import r2_score as r2\n",
    "\n",
    "train_score = model.evaluate(X_train, Y_train, verbose=1)\n",
    "print('Train loss:', train_score)\n",
    "print('Train R2:', r2(Y_train, model.predict(X_train)))\n",
    "\n",
    "test_score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test loss:', test_score)\n",
    "print('Test R2:', r2(Y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Plot the train/test performace against the number of hidden nodes, H.\n",
    "\n",
    "**Hint**\n",
    "You should loop over enough neurons to make a decent plot.  However, if you make $H$ too big then it will take a long time to get a solution.  Choosing $H = [2, 4, 6, 8, 10]$ may be a good place to start.  You may want to try a larger list at some point in your free time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Deeper:  Changing the Number of Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fix a width $H$ and let's fit a MLP network with **multiple** hidden layers, each with the same width. Start with sigmoid or $tanh$ activation functions for the hidden nodes and linear activation for the output. \n",
    "\n",
    "***Experiment with the number of layers and observe the effect of this on the quality of the fit.***  You want to think about issues like computational effeciency and generalizability of this type of modeling. You should compare the MLP to your model with a single hidden layer (in terms of quality of fit, efficiency and generalizability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of hidden nodes\n",
    "H = 4\n",
    "# input dimension\n",
    "input_dim = 1\n",
    "\n",
    "# create sequential multi-layer perceptron\n",
    "model2 = Sequential()\n",
    "# layer 0\n",
    "# model2.add(...) \n",
    "\n",
    "# layer 1\n",
    "# model2.add(...) \n",
    "\n",
    "# layer 2\n",
    "# model2.add(...) \n",
    "\n",
    "# layer 3\n",
    "# model2.add(...) \n",
    "\n",
    "# layer 4\n",
    "# model2.add(...) \n",
    "\n",
    "# layer 5\n",
    "# model2.add(...) \n",
    "\n",
    "# layer 6\n",
    "# model2.add(...) \n",
    "\n",
    "# configure the model\n",
    "# model2.compile(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "# model2.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our model to predict in the range we want\n",
    "X_range = np.linspace(-4, 4, 500)\n",
    "y_pred = model2.predict(X_range)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "ax.scatter(X_train, Y_train, label='Training data')\n",
    "ax.plot(X_range, y_pred, lw=4, color='r', label='MLP with 6 hidden layers')\n",
    "\n",
    "ax.set_xlabel(r'$X$', fontsize=20)\n",
    "ax.set_ylabel(r'$Y$', fontsize=20)\n",
    "ax.set_title('Toy data set for regression', fontsize=24)\n",
    "ax.tick_params(labelsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the training and testing performance of your model \n",
    "# note: you should extract check both the loss function and your evaluation metric\n",
    "score = model2.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Train loss:', score)\n",
    "print('Train R2:', r2(Y_train, model2.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model2.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score)\n",
    "print('Test R2:', r2(Y_test, model2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "What if we wanted to approximate a different function $f$ with MLP's?  Consider the functions $f_{1}\\left(x\\right) = x\\sin\\left(x\\right)$ and $f_{2}\\left(x\\right) = \\left|x\\right|$.  Can you create a network that can represent these functions?\n",
    "\n",
    "***Experiment with approximating a few different non-linear functions with wide but shallow networks as well as deep but narrow networks.***\n",
    "\n",
    "How expressive do you think MLP's are?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation Demo\n",
    "Lecture introduced automatic differentiation.  Here we will show a *very basic* implementation for the forward mode.\n",
    "\n",
    "Suppose we want to compute the derivative of $f = x^{3}$.  This is a polynomial (actually a monomial) function and polynomials are *elementary* (or elemental) functions.\n",
    "\n",
    "We'd like to evaluate $f^{\\prime}$ at the point $a = 3$.  We can do this by hand in this simple example:\n",
    "\\begin{align}\n",
    "  &f\\left(3\\right) = 27 \\\\\n",
    "  &f^{\\prime}\\left(3\\right) = 3 \\left(3^{2}\\right) = 27\n",
    "\\end{align}\n",
    "\n",
    "Let's create a very simple `Python` datastructure to handle this case.\n",
    "\n",
    "A list comes to mind.  The first index of the list will be the function value and the second index will be the derivative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = [0, 0] # Initialize the list\n",
    "a = 3.0 # The point we want to evaluate things at\n",
    "f[0] = a**3.0\n",
    "f[1] = 3.0 * a**2.0\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked great!  The basic idea was that we can create a library of elementary functions and their derivatives.  Whenever `python` sees that elementary function, it automatically calculates the derivative as well.  This works very well in the forward mode.  Here's a slightly more advanced implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardAD:\n",
    "    def __init__(self, val, der=1.0):\n",
    "        self.val = val\n",
    "        self.der = der\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        try:\n",
    "            return ForwardAD(self.val + other.val, self.der + other.der)\n",
    "        except:\n",
    "            return ForwardAD(self.val + other, self.der)\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return ForwardAD(self.val + other, self.der)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        try:\n",
    "            return ForwardAD(self.val * other.val, self.val * other.der + self.der * other.val)\n",
    "        except:\n",
    "            return ForwardAD(self.val * other, self.der * other)\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return ForwardAD(self.val * other, self.der * other)\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        try:\n",
    "            fa = self.val**other.val\n",
    "            fpa = (other.val / self.val * self.der + np.log(self.val) * other.der) * fa\n",
    "            return ForwardAD(fa, fpa)\n",
    "        except:\n",
    "            fa = self.val**other\n",
    "            fpa = other * self.val**(other - 1.0) * self.der\n",
    "            return ForwardAD(fa, fpa)\n",
    "    \n",
    "    def __rpow__(self, other):\n",
    "        fa = self.val**other\n",
    "        fpa = fa * np.log(other) * self.der\n",
    "        return ForwardAD(fa, fpa)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ForwardAD(3.0)\n",
    "y = x**3.0 + 2.0 * x**2.0 + 3.0\n",
    "print(y.val, y.der)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
